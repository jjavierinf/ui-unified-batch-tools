# config.yaml
expected_workload: high
#   - "high" => Use Spark with 4 cores for high-performance tasks.
#   - "medium" => Use Spark with 2 cores for moderate workloads.
#   - "low" => Use Python for lightweight tasks with minimal resource usage.
# Set the expected workload to control the execution engine and resource allocation.expected_workload: high 

source:
  type: sql_server  # postgres or sql_server
  conn: sqlserver_Accounts_conn
  table: AccountReference
  schema: dbo
  query: SELECT * FROM dbo.AccountReference
  source_timezone: US/Eastern
  incremental_update_field: None 

target:
  type: starrocks
  database: db_stage
  table: Accounts_dbo_AccountReference

dag_type: snapshot  # Specify the job type: 'snapshot' or 'incremental'

dag_config:
  dag_name: dag_CRM_integration_dbo_AccountReference
  integration_name: CRM_integration
  dag_description: DAG to replicate Accounts AccountReference for CRM_integration
  dag_schedule_interval: 7 0,3,6,9,12,15,18,21 * * *
  dag_catchup: False
  dag_max_active_runs: 1
  dag_tags:
    - CRM_integration
    - snapshot
  dag_owner: a.secreti
  dag_start_date: "2023-11-28T00:00:00"